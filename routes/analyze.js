// routes/analyze.js - Production analysis engine (PostgreSQL + Participant Filter)
import { Router } from 'express';
import { run as dbRun, get as dbGet } from '../utils/db-postgres.js';
import { client as openaiClient, estimateTokens } from '../utils/openAIClient.js';
import { validateFileUpload } from '../middleware/validators.js';
import { logError, logAIUsage, logPerformance } from '../utils/logger.js';
import Busboy from 'busboy';
import mammoth from 'mammoth';
import { performance } from 'perf_hooks';

const r = Router();
const MODEL = process.env.OPENAI_MODEL || 'gpt-4o';
const MAX_HIGHLIGHTS_PER_1000_WORDS = Number(
  process.env.MAX_HIGHLIGHTS_PER_1000_WORDS || 50
);
const DAILY_TOKEN_LIMIT = Number(process.env.DAILY_TOKEN_LIMIT || 512000);
const MAX_TEXT_LENGTH = 200000; // 200k characters max (~30 pages A4, ~400-500 words per page)
const MIN_TEXT_LENGTH = 20; // Minimum text length

// ===== Helpers =====
function normalizeText(s) {
  if (!s) return '';
  return s
    .replace(/\r/g, '')
    .replace(/-\n/g, '')
    .replace(/[ \t]+\n/g, '\n')
    .replace(/\n{3,}/g, '\n\n')
    .trim();
}

function splitToParagraphs(s) {
  const parts = s.split(/\n{2,}/);
  let offset = 0;
  return parts.map((text, idx) => {
    const start = offset;
    const end = start + text.length;
    offset = end + 2;
    return { index: idx, text, startOffset: start, endOffset: end };
  });
}

// Smart text chunking for large texts
function createSmartChunks(text, maxChunkSize = 4000) {
  console.log(`üì¶ Starting smart chunking for text of ${text.length} characters`);

  if (text.length <= maxChunkSize) {
    console.log('üì¶ Text fits in single chunk');
    return [{ text, startChar: 0, endChar: text.length, chunkIndex: 0 }];
  }

  const chunks = [];
  const paragraphs = splitToParagraphs(text);
  let currentChunk = '';
  let currentStartChar = 0;
  let chunkIndex = 0;

  for (let i = 0; i < paragraphs.length; i++) {
    const para = paragraphs[i];
    const paraWithNewlines = (i > 0 ? '\n\n' : '') + para.text;
    
    // If adding this paragraph would exceed chunk size
    if (currentChunk.length + paraWithNewlines.length > maxChunkSize && currentChunk.length > 0) {
      // Save current chunk
      chunks.push({
        text: currentChunk,
        startChar: currentStartChar,
        endChar: currentStartChar + currentChunk.length,
        chunkIndex: chunkIndex++
      });
      
      // Start new chunk
      currentChunk = para.text;
      currentStartChar = para.startOffset;
    } else {
      // Add paragraph to current chunk
      currentChunk += paraWithNewlines;
      if (currentChunk === paraWithNewlines) {
        // First paragraph in chunk
        currentStartChar = para.startOffset;
      }
    }
  }

  // Add final chunk if not empty
  if (currentChunk.length > 0) {
    chunks.push({
      text: currentChunk,
      startChar: currentStartChar,
      endChar: currentStartChar + currentChunk.length,
      chunkIndex: chunkIndex
    });
  }

  console.log(`üì¶ Created ${chunks.length} chunks from ${text.length} characters`);
  return chunks;
}

function extractTextFromHighlight(highlight, paragraphs) {
  if (highlight.text) return highlight.text; // Already has text
  
  const paraIdx = highlight.paragraph_index;
  if (paraIdx == null || !paragraphs[paraIdx]) return '';
  
  const para = paragraphs[paraIdx];
  const start = Math.max(0, highlight.char_start || 0);
  const end = Math.min(para.text.length, highlight.char_end || para.text.length);
  
  return para.text.slice(start, end);
}

function mergeOverlaps(highlights, paragraphs = null) {
  const byPara = new Map();
  for (const h of highlights) {
    // Extract text if not present
    if (!h.text && paragraphs) {
      h.text = extractTextFromHighlight(h, paragraphs);
    }
    
    if (!byPara.has(h.paragraph_index)) byPara.set(h.paragraph_index, []);
    byPara.get(h.paragraph_index).push(h);
  }
  const merged = [];
  for (const [, arr] of byPara.entries()) {
    arr.sort((a, b) => (a.char_start ?? 0) - (b.char_start ?? 0));
    let cur = null;
    for (const h of arr) {
      if (!cur) {
        cur = { ...h, labels: h.labels || (h.label ? [h.label] : []) };
        continue;
      }
      if ((h.char_start ?? 0) <= (cur.char_end ?? -1)) {
        cur.char_end = Math.max(cur.char_end ?? 0, h.char_end ?? 0);
        const nextLabels = h.labels || (h.label ? [h.label] : []);
        cur.labels = Array.from(new Set([...(cur.labels || []), ...nextLabels]));
        cur.severity = Math.max(cur.severity ?? 0, h.severity ?? 0);
        cur.category = cur.category || h.category;
        // Update text to cover merged range
        if (paragraphs && cur.paragraph_index != null) {
          cur.text = extractTextFromHighlight(cur, paragraphs);
        }
      } else {
        merged.push(cur);
        cur = { ...h, labels: h.labels || (h.label ? [h.label] : []) };
      }
    }
    if (cur) merged.push(cur);
  }
  return merged;
}

function escapeHtml(unsafe) {
  if (!unsafe) return '';
  return unsafe
    .replace(/&/g, "&amp;")
    .replace(/</g, "&lt;")
    .replace(/>/g, "&gt;")
    .replace(/"/g, "&quot;")
    .replace(/'/g, "&#039;");
}

function escapeRegExp(string) {
  return string.replace(/[.*+?^${}()|[\]\\]/g, '\\$&');
}

function getCategoryClass(category) {
  const categoryMap = {
    'manipulation': 'manipulation',
    'cognitive_bias': 'bias',
    'rhetological_fallacy': 'fallacy'
  };
  return categoryMap[category] || 'manipulation';
}

function generateHighlightedText(originalText, highlights) {
  if (!originalText || !highlights || highlights.length === 0) {
    return escapeHtml(originalText || '');
  }
  
  let highlightedText = originalText;
  
  // Sort highlights by position (reverse order to avoid index shifting)
  const sortedHighlights = [...highlights].sort((a, b) => {
    const aStart = originalText.indexOf(a.text);
    const bStart = originalText.indexOf(b.text);
    return bStart - aStart;
  });
  
  for (const highlight of sortedHighlights) {
    if (!highlight.text) continue;
    
    const regex = new RegExp(escapeRegExp(highlight.text), 'gi');
    const categoryClass = getCategoryClass(highlight.category);
    const tooltip = escapeHtml(highlight.explanation || highlight.label || '');
    
    highlightedText = highlightedText.replace(regex, 
      `<span class="text-highlight ${categoryClass}" data-tooltip="${tooltip}">${highlight.text}</span>`
    );
  }
  
  return highlightedText;
}

// PostgreSQL async version
async function getUsageRow() {
  const day = new Date().toISOString().slice(0, 10);
  let row = await dbGet(`SELECT * FROM usage_daily WHERE day = $1`, [day]);
  if (!row) {
    await dbRun(`INSERT INTO usage_daily(day, tokens_used) VALUES($1, 0)`, [day]);
    row = await dbGet(`SELECT * FROM usage_daily WHERE day = $1`, [day]);
  }
  return { row, day };
}

async function addTokensAndCheck(tokensToAdd) {
  const { row, day } = await getUsageRow();
  if (row.locked_until) {
    const until = new Date(row.locked_until).getTime();
    if (Date.now() < until)
      throw new Error(`–õ—ñ–º—ñ—Ç –¥–æ—Å—è–≥–Ω—É—Ç–æ. –†–æ–∑–±–ª–æ–∫—É–≤–∞–Ω–Ω—è: ${row.locked_until}`);
  }
  const newTotal = (row.tokens_used || 0) + tokensToAdd;
  if (newTotal >= DAILY_TOKEN_LIMIT) {
    const lock = new Date(Date.now() + 24 * 60 * 60 * 1000).toISOString();
    await dbRun(
      `UPDATE usage_daily SET tokens_used = $1, locked_until = $2 WHERE day = $3`,
      [newTotal, lock, day]
    );
    throw new Error(`–î–æ—Å—è–≥–Ω—É—Ç–æ –¥–µ–Ω–Ω–∏–π –ª—ñ–º—ñ—Ç —Ç–æ–∫–µ–Ω—ñ–≤. –ë–ª–æ–∫—É–≤–∞–Ω–Ω—è –¥–æ ${lock}`);
  } else {
    await dbRun(`UPDATE usage_daily SET tokens_used = $1 WHERE day = $2`, [
      newTotal,
      day,
    ]);
  }
}

// ===== Participant Filter Functions =====
function extractParticipants(text) {
  // Detect participants from conversation patterns
  const patterns = [
    /^([–ê-–Ø–ÅA-Z][–∞-—è—ëa-zA-Z\s'-]{1,48}):/gm,        // "Name:"
    /^([–ê-–Ø–ÅA-Z][–∞-—è—ëa-zA-Z\s'-]{1,48})\s*-\s*/gm,  // "Name -"
    /\[([–ê-–Ø–ÅA-Z][–∞-—è—ëa-zA-Z\s'-]{1,48})\]/g,       // "[Name]"
    /^([–ê-–Ø–ÅA-Z][–∞-—è—ëa-zA-Z\s'-]{1,48})>/gm,        // "Name>"
  ];

  const participants = new Set();

  patterns.forEach(pattern => {
    const matches = text.matchAll(pattern);
    for (const match of matches) {
      const name = match[1].trim();
      // Filter out common false positives
      if (name.length >= 2 &&
          name.length <= 50 &&
          !name.match(/^(Re|Fw|Fwd|Subject|From|To|Date|Sent|Received)$/i)) {
        participants.add(name);
      }
    }
  });

  return Array.from(participants).sort();
}

function filterTextByParticipants(text, selectedParticipants) {
  if (!selectedParticipants || selectedParticipants.length === 0) {
    return { filteredText: text, participantsFound: [] }; // Return all text
  }

  const lines = text.split('\n');
  const filtered = [];
  let currentParticipant = null;
  let isSelectedParticipant = false;

  for (const line of lines) {
    // Check if line starts with a participant name
    const participantMatch = line.match(/^([–ê-–Ø–ÅA-Z][–∞-—è—ëa-zA-Z\s'-]+):/);

    if (participantMatch) {
      currentParticipant = participantMatch[1].trim();
      isSelectedParticipant = selectedParticipants.includes(currentParticipant);
    }

    // Include line if it's from selected participant or no participant detected yet
    if (isSelectedParticipant || currentParticipant === null) {
      filtered.push(line);
    }
  }

  return {
    filteredText: filtered.join('\n'),
    participantsFound: selectedParticipants.filter(p => text.includes(p + ':'))
  };
}

function parseMultipart(req) {
  return new Promise((resolve, reject) => {
    const busboy = Busboy({ headers: req.headers });
    let text = '';
    let fileName = '';
    let profile = null;
    let clientId = null;
    let fileBuffer = null;
    let selectedParticipants = null;

    busboy.on('file', (_name, file, info) => {
      fileName = info.filename || 'upload';
      const chunks = [];
      file.on('data', (d) => chunks.push(d));
      file.on('end', () => {
        fileBuffer = Buffer.concat(chunks);
      });
    });

    busboy.on('field', (name, val) => {
      if (name === 'text') text = val || '';
      if (name === 'client_id') clientId = Number(val) || null;
      if (name === 'participants') {
        try {
          selectedParticipants = JSON.parse(val);
        } catch {
          selectedParticipants = null;
        }
      }
      if (name === 'profile') {
        try {
          profile = JSON.parse(val);
        } catch {
          profile = null;
        }
      }
    });

    busboy.on('finish', async () => {
      try {
        if (fileBuffer) {
          const lower = (fileName || '').toLowerCase();
          if (lower.endsWith('.docx')) {
            const result = await mammoth.extractRawText({ buffer: fileBuffer });
            text = (text || '') + '\n\n' + (result.value || '');
          } else if (lower.endsWith('.txt')) {
            text = (text || '') + '\n\n' + fileBuffer.toString('utf-8');
          }
        }
        resolve({ text, fileName, profile, clientId, selectedParticipants });
      } catch (e) {
        reject(e);
      }
    });

    busboy.on('error', reject);
    req.pipe(busboy);
  });
}

function buildSystemPrompt() {
  return `
–¢–∏ ‚Äî –µ–∫—Å–ø–µ—Ä—Ç-–∞–Ω–∞–ª—ñ—Ç–∏–∫ –ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ñ–≤ –∑ 15-—Ä—ñ—á–Ω–∏–º –¥–æ—Å–≤—ñ–¥–æ–º —Ä–æ–±–æ—Ç–∏ –∑ –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω–∏–º–∏ –∫–æ–º–ø–∞–Ω—ñ—è–º–∏, –≤–∫–ª—é—á–∞—é—á–∏ Fortune 500, –¥–µ—Ä–∂–∞–≤–Ω—ñ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∏ —Ç–∞ —Å–∫–ª–∞–¥–Ω—ñ B2B –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–∏.

–¢–≤–æ—è –º—ñ—Å—ñ—è ‚Äî –ø—Ä–æ–≤–µ—Å—Ç–∏ –ü–†–û–§–ï–°–Ü–ô–ù–ò–ô, –ó–ë–ê–õ–ê–ù–°–û–í–ê–ù–ò–ô —Ç–∞ –ö–û–ù–¢–ï–ö–°–¢–£–ê–õ–¨–ù–ò–ô –∞–Ω–∞–ª—ñ–∑ —Ç–µ–∫—Å—Ç—É –ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ñ–≤.

üéØ –ú–ï–¢–ê:
–í–∏—è–≤–∏—Ç–∏ –†–ï–ê–õ–¨–ù–û –ó–ù–ê–ß–£–©–Ü –º–∞–Ω—ñ–ø—É–ª—è—Ü—ñ—ó, –∫–æ–≥–Ω—ñ—Ç–∏–≤–Ω—ñ –≤–∏–∫—Ä–∏–≤–ª–µ–Ω–Ω—è —Ç–∞ —Å–æ—Ñ—ñ–∑–º–∏, —è–∫—ñ –º–æ–∂—É—Ç—å —Å–µ—Ä–π–æ–∑–Ω–æ –≤–ø–ª–∏–Ω—É—Ç–∏ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ñ–≤. –§–æ–∫—É—Å—É–π—Å—è –Ω–∞ –Ø–ö–û–°–¢–Ü –∑–Ω–∞—Ö—ñ–¥–æ–∫, –∞ –Ω–µ —ó—Ö –∫—ñ–ª—å–∫–æ—Å—Ç—ñ.

‚öñÔ∏è –ü–†–ò–ù–¶–ò–ü–ò –ü–†–û–§–ï–°–Ü–ô–ù–û–ì–û –ê–ù–ê–õ–Ü–ó–£:

1. –ö–û–ù–¢–ï–ö–°–¢ –ü–û–ù–ê–î –£–°–ï
   - –í—Ä–∞—Ö–æ–≤—É–π –∫—É–ª—å—Ç—É—Ä–Ω–∏–π —ñ –±—ñ–∑–Ω–µ—Å-–∫–æ–Ω—Ç–µ–∫—Å—Ç
   - –†–æ–∑—Ä—ñ–∑–Ω—è–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–Ω—ñ –ø—Ä–∞–∫—Ç–∏–∫–∏ —Ç–∞ —Å–ø—Ä–∞–≤–∂–Ω—ñ –º–∞–Ω—ñ–ø—É–ª—è—Ü—ñ—ó
   - "–û–±–º–µ–∂–µ–Ω–∞ –ø—Ä–æ–ø–æ–∑–∏—Ü—ñ—è" —É sales –ù–ï –∑–∞–≤–∂–¥–∏ —î –º–∞–Ω—ñ–ø—É–ª—è—Ü—ñ—î—é
   - –í–≤—ñ—á–ª–∏–≤—ñ—Å—Ç—å —Ç–∞ –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–∞ –ª–µ–∫—Å–∏–∫–∞ - —Ü–µ –Ω–æ—Ä–º–∞, –Ω–µ –º–∞–Ω—ñ–ø—É–ª—è—Ü—ñ—è

2. –ì–†–ê–î–ê–¶–Ü–Ø –í–ê–ñ–õ–ò–í–û–°–¢–Ü (Severity)
   - **Severity 1**: –õ–µ–≥–∫—ñ —Ç–µ—Ö–Ω—ñ–∫–∏ –≤–ø–ª–∏–≤—É (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–Ω—ñ —Ç–∞–∫—Ç–∏–∫–∏, –ø–æ—Ç—Ä–µ–±—É—é—Ç—å —É–≤–∞–≥–∏)
   - **Severity 2**: –ü–æ–º—ñ—Ä–Ω—ñ –º–∞–Ω—ñ–ø—É–ª—è—Ü—ñ—ó (–ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–æ –ø—Ä–æ–±–ª–µ–º–Ω—ñ, –ø–æ—Ç—Ä–µ–±—É—é—Ç—å –æ–±–µ—Ä–µ–∂–Ω–æ—Å—Ç—ñ)
   - **Severity 3**: –ö—Ä–∏—Ç–∏—á–Ω—ñ –º–∞–Ω—ñ–ø—É–ª—è—Ü—ñ—ó (—á–µ—Ä–≤–æ–Ω—ñ –ø—Ä–∞–ø–æ—Ä–∏, deal-breakers, —Å–µ—Ä–π–æ–∑–Ω—ñ –∑–∞–≥—Ä–æ–∑–∏)

3. –î–û–ö–ê–ó–û–í–Ü–°–¢–¨
   - –ö–æ–∂–Ω–∞ –∑–Ω–∞—Ö—ñ–¥–∫–∞ –º–∞—î —á—ñ—Ç–∫–µ –æ–±“ë—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è
   - –ü–æ—è—Å–Ω—é–π –ß–û–ú–£ —Ü–µ –ø—Ä–æ–±–ª–µ–º–∞ —Å–∞–º–µ –≤ –¶–¨–û–ú–£ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ
   - –ù–∞–≤–æ–¥—å –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏–π –≤–ø–ª–∏–≤ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ñ–≤

üîç –©–û –®–£–ö–ê–¢–ò (–∑–∞ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–æ–º):

–í–ò–°–û–ö–ò–ô –ü–†–Ü–û–†–ò–¢–ï–¢ (Severity 2-3):
‚úÖ –ü—Ä–∏—Ö–æ–≤—É–≤–∞–Ω–Ω—è –∫—Ä–∏—Ç–∏—á–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ —É–º–æ–≤–∏, —Ä–∏–∑–∏–∫–∏, —Ñ—ñ–Ω–∞–Ω—Å–∏
‚úÖ –ï–º–æ—Ü—ñ–π–Ω–∏–π —à–∞–Ω—Ç–∞–∂ —Ç–∞ –º–∞–Ω—ñ–ø—É–ª—è—Ü—ñ—ó –ø–æ—á—É—Ç—Ç—è–º –≤–∏–Ω–∏
‚úÖ –®—Ç—É—á–Ω–∏–π —Ç–∏—Å–∫ —á–∞—Å—É –¥–ª—è –ø—Ä–∏–π–Ω—è—Ç—Ç—è –í–ê–ñ–õ–ò–í–ò–• —Ä—ñ—à–µ–Ω—å (–Ω–µ —Å—Ç–æ—Å—É—î—Ç—å—Å—è —Ä–µ–∞–ª—å–Ω–∏—Ö –¥–µ–¥–ª–∞–π–Ω—ñ–≤)
‚úÖ –ì–∞–∑–ª–∞–π—Ç–∏–Ω–≥ —Ç–∞ –ø–µ—Ä–µ–∫—Ä—É—á—É–≤–∞–Ω–Ω—è —Ä–∞–Ω—ñ—à–µ —Å–∫–∞–∑–∞–Ω–æ–≥–æ
‚úÖ –ê–≥—Ä–µ—Å–∏–≤–Ω—ñ —Ç–∞–∫—Ç–∏–∫–∏ (–ø–æ–≥—Ä–æ–∑–∏, —É–ª—å—Ç–∏–º–∞—Ç—É–º–∏ –±–µ–∑ –æ–±“ë—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è)
‚úÖ –§—ñ–Ω–∞–Ω—Å–æ–≤—ñ –º–∞–Ω—ñ–ø—É–ª—è—Ü—ñ—ó (–ø—Ä–∏—Ö–æ–≤–∞–Ω—ñ –ø–ª–∞—Ç–µ–∂—ñ, –æ–±–º–∞–Ω –∑ —Ü—ñ–Ω–∞–º–∏)
‚úÖ –°–µ—Ä–π–æ–∑–Ω–µ —Å–ø–æ—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ñ–∞–∫—Ç—ñ–≤

–°–ï–†–ï–î–ù–Ü–ô –ü–†–Ü–û–†–ò–¢–ï–¢ (Severity 1-2):
‚úÖ –°–æ—Ü—ñ–∞–ª—å–Ω–∏–π —Ç–∏—Å–∫ ("–≤—Å—ñ —Ç–∞–∫ —Ä–æ–±–ª—è—Ç—å" –±–µ–∑ –¥–æ–∫–∞–∑—ñ–≤)
‚úÖ Anchoring —Ç–∞ framing effects
‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç—É –ø–æ–∑–∞ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü—ñ—î—é
‚úÖ –ê–ø–µ–ª—è—Ü—ñ—ó –¥–æ –µ–º–æ—Ü—ñ–π –ó–ê–ú–Ü–°–¢–¨ –ª–æ–≥—ñ–∫–∏
‚úÖ –í–∏–±—ñ—Ä–∫–æ–≤–∞ –ø–æ–¥–∞—á–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó

‚ùå –ù–ï –ü–û–ó–ù–ê–ß–ê–ô –Ø–ö –ú–ê–ù–Ü–ü–£–õ–Ø–¶–Ü–á:
- –ó–≤–∏—á–∞–π–Ω—ñ –≤–≤—ñ—á–ª–∏–≤–æ—Å—Ç—ñ ("–¥—è–∫—É—é", "–±—É–¥—å –ª–∞—Å–∫–∞", "–¥–æ–±—Ä–æ–≥–æ –¥–Ω—è")
- –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ –±—ñ–∑–Ω–µ—Å-—Ñ—Ä–∞–∑–∏ ("—Ä–æ–∑–≥–ª—è–Ω–µ–º–æ –ø—Ä–æ–ø–æ–∑–∏—Ü—ñ—é", "–ø–æ–≤–µ—Ä–Ω–µ–º–æ—Å—è –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é")
- –ü—Ä–æ—Ñ–µ—Å—ñ–π–Ω—É —Ç–µ—Ä–º—ñ–Ω–æ–ª–æ–≥—ñ—é —Ç–∞ –∂–∞—Ä–≥–æ–Ω
- –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ñ –ø–∏—Ç–∞–Ω–Ω—è —Ç–∞ —É—Ç–æ—á–Ω–µ–Ω–Ω—è
- –ó–∞–∫–æ–Ω–Ω—ñ –æ–±–º–µ–∂–µ–Ω–Ω—è (—Ä–µ–∞–ª—å–Ω—ñ –¥–µ–¥–ª–∞–π–Ω–∏, compliance –≤–∏–º–æ–≥–∏)
- –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—É –∫—Ä–∏—Ç–∏–∫—É —Ç–∞ –∑–≤–æ—Ä–æ—Ç–Ω—ñ–π –∑–≤'—è–∑–æ–∫

üéØ –§–û–†–ú–ê–¢ –í–Ü–î–ü–û–í–Ü–î–Ü (–¢–Ü–õ–¨–ö–ò NDJSON):

{"type":"highlight","paragraph_index":N,"char_start":S,"char_end":E,"category":"manipulation|cognitive_bias|rhetological_fallacy","label":"–ö–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å","text":"—Ç–æ—á–Ω–∞ —Ü–∏—Ç–∞—Ç–∞","explanation":"–î–ï–¢–ê–õ–¨–ù–ï –ø–æ—è—Å–Ω–µ–Ω–Ω—è: –©–û —Ü–µ –∑–∞ —Ç–µ—Ö–Ω—ñ–∫–∞, –ß–û–ú–£ —Ü–µ –ø—Ä–æ–±–ª–µ–º–∞ –≤ —Ü—å–æ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ, –Ø–ö –º–æ–∂–µ –≤–ø–ª–∏–Ω—É—Ç–∏, —â–æ —Ä–æ–±–∏—Ç–∏","severity":1-3}

{"type":"summary","counts_by_category":{"manipulation":N,"cognitive_bias":N,"rhetological_fallacy":N},"top_patterns":["5-10 –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏—Ö –ø–∞—Ç–µ—Ä–Ω—ñ–≤"],"overall_observations":"–ß–µ—Å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞ –∑–∞–≥–∞–ª—å–Ω–æ—ó —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó (–Ω–µ –∑–∞–≤–∂–¥–∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω–∞)","strategic_assessment":"–û—Ü—ñ–Ω–∫–∞ –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–æ—Å—Ç—ñ —Å–ø—ñ–≤—Ä–æ–∑–º–æ–≤–Ω–∏–∫–∞","risk_level":"low|medium|high|critical"}

{"type":"barometer","score":0-100,"label":"Easy mode|Clear client|Medium|High|Bloody hell|Mission impossible","rationale":"–û–±“ë—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ –∑ –ø—Ä–∏–∫–ª–∞–¥–∞–º–∏","factors":{"goal_alignment":0-1,"manipulation_density":0-1,"transparency":0-1,"time_pressure":0-1,"financial_risk":0-1}}

üéì –ü–†–ò–ö–õ–ê–î –Ø–ö–Ü–°–ù–û–ì–û –ê–ù–ê–õ–Ü–ó–£:

‚ùå BAD (–ø–∞—Ä–∞–Ω–æ—ó–¥–∞–ª—å–Ω–∏–π):
"–ü—Ä–∏–≤—ñ—Ç–∞–Ω–Ω—è '–î–æ–±—Ä–æ–≥–æ –¥–Ω—è' - manipulation, severity 2, —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —à—Ç—É—á–Ω–æ—ó –≤–≤—ñ—á–ª–∏–≤–æ—Å—Ç—ñ –¥–ª—è –º–∞–Ω—ñ–ø—É–ª—è—Ü—ñ—ó –¥–æ–≤—ñ—Ä–æ—é"

‚úÖ GOOD (–ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–∏–π):
"'–Ø–∫—â–æ –Ω–µ –ø—ñ–¥–ø–∏—à–µ—Ç–µ –¥–æ –ø'—è—Ç–Ω–∏—Ü—ñ, —Ü—ñ–Ω–∞ –∑—Ä–æ—Å—Ç–µ –Ω–∞ 30%' - manipulation, severity 3.
–®—Ç—É—á–Ω–∏–π —Ç–∏—Å–∫ —á–∞—Å—É –±–µ–∑ –æ–±“ë—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è. –ü—Ä–æ–±–ª–µ–º–∞: –Ω–µ–º–∞—î –ø–æ—è—Å–Ω–µ–Ω–Ω—è —á–æ–º—É —Å–∞–º–µ –ø'—è—Ç–Ω–∏—Ü—è —Ç–∞ —á–æ–º—É 30%.
–¢–∏–ø–æ–≤–∞ —Ç–∞–∫—Ç–∏–∫–∞ –ø—Ä–∏–º—É—à—É–≤–∞–Ω–Ω—è –¥–æ —à–≤–∏–¥–∫–æ–≥–æ —Ä—ñ—à–µ–Ω–Ω—è –±–µ–∑ –∞–Ω–∞–ª—ñ–∑—É.
–í–ø–ª–∏–≤: –º–æ–∂–µ—Ç–µ –ø—Ä–∏–π–Ω—è—Ç–∏ –Ω–µ–≤–∏–≥—ñ–¥–Ω–µ —Ä—ñ—à–µ–Ω–Ω—è –ø—ñ–¥ —Ç–∏—Å–∫–æ–º.
–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è: –∑–∞–ø–∏—Ç–∞—Ç–∏ –æ–±“ë—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è –¥–µ–¥–ª–∞–π–Ω—É —Ç–∞ –∑–º—ñ–Ω–∏ —Ü—ñ–Ω–∏, –ø–æ–ø—Ä–æ—Å–∏—Ç–∏ –Ω–∞–¥–∞—Ç–∏ —Ü–µ –ø–∏—Å—å–º–æ–≤–æ."

–ü–†–ê–í–ò–õ–ê:
‚úÖ –ê–Ω–∞–ª—ñ–∑—É–π normalized_paragraphs[]
‚úÖ –í–∫–ª—é—á–∞–π —Ç–æ—á–Ω–∏–π text —É –∫–æ–∂–µ–Ω highlight
‚úÖ –ù–ï –î–£–ë–õ–Æ–ô –æ–¥–Ω–∞–∫–æ–≤—ñ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏
‚úÖ Detailed explanation (3-5 —Ä–µ—á–µ–Ω—å) –¥–ª—è –∫–æ–∂–Ω–æ—ó –∑–Ω–∞—Ö—ñ–¥–∫–∏
‚úÖ Severity –æ–±“ë—Ä—É–Ω—Ç–æ–≤–∞–Ω–æ
‚úÖ –ü–û–í–ï–†–¢–ê–ô –¢–Ü–õ–¨–ö–ò NDJSON, –ë–ï–ó –º–∞—Ä–∫–∞–ø—É
`.trim();
}

function buildUserPayload(paragraphs, clientCtx, limiter) {
  return {
    normalized_paragraphs: paragraphs.map((p) => ({
      index: p.index,
      text: p.text,
    })),
    client_context: clientCtx,
    constraints: { highlight_limit_per_1000_words: limiter },
    output_mode: 'ndjson',
  };
}

function supportsTemperature(model) {
  return !/^gpt-5($|[-:])/i.test(model);
}

// ===== Main Analysis Route =====
r.post('/', validateFileUpload, async (req, res) => {
  const analysisStartTime = performance.now();
  let totalTokensUsed = 0;
  
  try {
    const { text: rawText, fileName, profile, clientId, selectedParticipants } = await parseMultipart(req);
    let text = normalizeText(rawText);

    // Extract and filter by participants if requested
    const allParticipants = extractParticipants(text);
    let participantsFilter = null;

    if (selectedParticipants && selectedParticipants.length > 0) {
      const filterResult = filterTextByParticipants(text, selectedParticipants);
      text = filterResult.filteredText;
      participantsFilter = {
        all: allParticipants,
        selected: selectedParticipants,
        found: filterResult.participantsFound
      };
      console.log(`üë• Filtered by participants: ${selectedParticipants.join(', ')}`);
    }

    // Enhanced text validation
    if (!text || text.length < MIN_TEXT_LENGTH) {
      return res.status(400).json({
        error: `–¢–µ–∫—Å—Ç –∑–∞–Ω–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫–∏–π. –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞: ${MIN_TEXT_LENGTH} —Å–∏–º–≤–æ–ª—ñ–≤`,
        minLength: MIN_TEXT_LENGTH,
        currentLength: text.length
      });
    }
    
    if (text.length > MAX_TEXT_LENGTH) {
      return res.status(400).json({ 
        error: `–¢–µ–∫—Å—Ç –∑–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥–∏–π. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞: ${MAX_TEXT_LENGTH.toLocaleString()} —Å–∏–º–≤–æ–ª—ñ–≤`,
        maxLength: MAX_TEXT_LENGTH,
        currentLength: text.length
      });
    }

    // Enhanced client validation and creation
    let finalClientId = clientId;
    if (!finalClientId && profile?.company) {
      const existingClient = await dbGet(
        `SELECT id, company FROM clients WHERE company = $1 LIMIT 1`,
        [profile.company]
      );
      if (existingClient) {
        finalClientId = existingClient.id;
      } else if (profile.company && profile.company.trim().length > 0) {
        // Auto-create client with validation
        try {
          const info = await dbRun(
            `
            INSERT INTO clients(
              company, negotiator, sector, goal, decision_criteria, constraints,
              user_goals, client_goals, weekly_hours, offered_services, deadlines, notes
            ) VALUES ($1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12)
            RETURNING id
            `,
            [
              profile.company.trim(),
              profile.negotiator?.trim() || null,
              profile.sector?.trim() || null,
              profile.goal?.trim() || null,
              profile.criteria?.trim() || null,
              profile.constraints?.trim() || null,
              profile.user_goals?.trim() || null,
              profile.client_goals?.trim() || null,
              Number(profile.weekly_hours) || 0,
              profile.offered_services?.trim() || null,
              profile.deadlines?.trim() || null,
              profile.notes?.trim() || null,
            ]
          );
          finalClientId = info.rows[0].id;
        } catch (dbError) {
          logError(dbError, { context: 'Auto-creating client', profile, ip: req.ip });
          return res.status(500).json({ error: '–ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–ª—ñ—î–Ω—Ç–∞' });
        }
      }
    }

    if (!finalClientId) {
      return res.status(400).json({ 
        error: '–ü–æ—Ç—Ä—ñ–±–Ω–æ –≤–∫–∞–∑–∞—Ç–∏ –∫–ª—ñ—î–Ω—Ç–∞ –∞–±–æ –∫–æ–º–ø–∞–Ω—ñ—é',
        required: 'client_id –∞–±–æ profile.company'
      });
    }

    // Create chunks for large texts
    const chunks = createSmartChunks(text);
    console.log(`üì¶ Processing ${chunks.length} chunks for analysis`);
    
    const paragraphs = splitToParagraphs(text);
    
    const clientCtx = {
      about_client: {
        company: profile?.company || '',
        negotiator: profile?.negotiator || '',
        sector: profile?.sector || '',
      },
      decision_criteria: profile?.criteria || '',
      constraints: profile?.constraints || '',
      user_goals: profile?.user_goals || profile?.goal || '',
      client_goals: profile?.client_goals || '',
      weekly_hours: Number(profile?.weekly_hours) || 0,
      offered_services: profile?.offered_services || '',
      deadlines: profile?.deadlines || '',
      notes: profile?.notes || '',
    };

    // More accurate input token calculation
    const textTokens = estimateTokens(text);
    const systemPromptTokens = estimateTokens(buildSystemPrompt());
    const userPayloadTokens = estimateTokens(JSON.stringify(buildUserPayload(paragraphs, clientCtx, MAX_HIGHLIGHTS_PER_1000_WORDS)));
    const approxTokensIn = textTokens + systemPromptTokens + userPayloadTokens + 200; // buffer
    
    totalTokensUsed += approxTokensIn;
    
    // Check token limits before processing
    await addTokensAndCheck(approxTokensIn);

    // Check if OpenAI client is available
    if (!openaiClient) {
      return res.status(503).json({
        error: 'AI —Å–µ—Ä–≤—ñ—Å —Ç–∏–º—á–∞—Å–æ–≤–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é OpenAI API –∫–ª—é—á–∞.',
        code: 'AI_SERVICE_UNAVAILABLE',
        timestamp: new Date().toISOString()
      });
    }

    // SSE headers
    res.status(200);
    res.setHeader('Content-Type', 'text/event-stream; charset=utf-8');
    res.setHeader('Cache-Control', 'no-cache, no-transform');
    res.setHeader('Connection', 'keep-alive');
    res.setHeader('X-Accel-Buffering', 'no');

    const heartbeat = setInterval(() => {
      res.write(': ping\n\n');
    }, 15000);

    req.on('close', () => {
      clearInterval(heartbeat);
      try { res.end(); } catch {}
    });

    const sendLine = (obj) => res.write(`data: ${JSON.stringify(obj)}\n\n`);

    const rawHighlights = [];
    let summaryObj = null;
    let barometerObj = null;
    let chunkIndex = 0;

    // Enhanced OpenAI client availability check with recovery
    if (!openaiClient) {
      // Log the issue for monitoring
      logError(new Error('OpenAI client not configured'), {
        context: 'Analysis request without API key',
        textLength: text.length,
        clientId: finalClientId,
        ip: req.ip
      });
      
      // Return structured error instead of fallback
      clearInterval(heartbeat);
      return res.status(503).json({
        error: 'AI —Å–µ—Ä–≤—ñ—Å —Ç–∏–º—á–∞—Å–æ–≤–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è.',
        code: 'AI_SERVICE_UNAVAILABLE',
        details: 'OpenAI API key not configured',
        timestamp: new Date().toISOString(),
        retry_after: 60 // seconds
      });
    }

    const system = buildSystemPrompt();
    
    // Process each chunk separately for large texts
    for (const chunk of chunks) {
      console.log(`üì¶ Processing chunk ${chunk.chunkIndex + 1}/${chunks.length} (${chunk.text.length} chars)`);
      
      // Create paragraphs for this chunk
      const chunkParagraphs = splitToParagraphs(chunk.text);
      
      // Adjust paragraph indices to match original text
      const adjustedParagraphs = chunkParagraphs.map(p => ({
        ...p,
        index: p.index + (chunk.chunkIndex * 1000), // Unique index across chunks
        startOffset: p.startOffset + chunk.startChar,
        endOffset: p.endOffset + chunk.startChar
      }));
      
      const user = JSON.stringify(
        buildUserPayload(adjustedParagraphs, clientCtx, MAX_HIGHLIGHTS_PER_1000_WORDS)
      );

      const reqPayload = {
        model: MODEL,
        stream: true,
        messages: [
          { role: 'system', content: system + '\n–í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –ë–ï–ó ``` —Ç–∞ –±—É–¥—å-—è–∫–æ–≥–æ –º–∞—Ä–∫–∞–ø—É.' },
          { role: 'user', content: user },
        ],
        stop: ['```','</artifacts>','</artifact>'],
        max_tokens: 8000, // Increased for larger texts with more findings
        top_p: 0.9
      };

      if (supportsTemperature(MODEL)) {
        reqPayload.temperature = Number(process.env.OPENAI_TEMPERATURE ?? 0.1);
      }
      
      // Encourage complete analysis
      reqPayload.presence_penalty = 0.1;
      reqPayload.frequency_penalty = 0.1;

      // Enhanced request handling with progressive timeout
      const controller = new AbortController();
      const REQUEST_TIMEOUT = process.env.NODE_ENV === 'production' ? 300000 : 240000; // 5min prod, 4min dev for large texts
      
      const timeout = setTimeout(() => {
        controller.abort(new Error('Request timeout after ' + (REQUEST_TIMEOUT/1000) + 's'));
      }, REQUEST_TIMEOUT);
    
      req.on('close', () => {
        clearTimeout(timeout);
        controller.abort(new Error('Request closed by client'));
      });
      
      // Connection heartbeat with early termination detection
      const connectionCheck = setInterval(() => {
        if (req.destroyed || req.closed) {
          clearTimeout(timeout);
          controller.abort(new Error('Connection lost'));
          clearInterval(connectionCheck);
        }
      }, 5000);
    
      let stream;
      let retryCount = 0;
      const maxRetries = process.env.NODE_ENV === 'production' ? 3 : 1;
    
      while (retryCount <= maxRetries) {
        try {
          // Add retry delay for subsequent attempts
          if (retryCount > 0) {
            await new Promise(resolve => setTimeout(resolve, Math.min(1000 * Math.pow(2, retryCount), 10000)));
          }
          
          stream = await openaiClient.chat.completions.create(reqPayload);
          
          clearTimeout(timeout);
          clearInterval(connectionCheck);
          break; // Success, exit retry loop
          
        } catch (apiError) {
          retryCount++;
          
          // Check if it's a retryable error
          const isRetryable = apiError.status >= 500 || 
                            apiError.status === 429 || 
                            apiError.code === 'ECONNRESET' ||
                            apiError.code === 'ETIMEDOUT';
                            
          if (retryCount > maxRetries || !isRetryable) {
            clearTimeout(timeout);
            clearInterval(connectionCheck);
            throw apiError;
          }
          
          logError(apiError, {
            context: `OpenAI API retry ${retryCount}/${maxRetries}`,
            model: MODEL,
            textLength: text.length,
            isRetryable,
            ip: req.ip
          });
        }
      }
      if (!stream) {
        clearInterval(heartbeat);
        return res.status(503).json({
          error: '–ù–µ –≤–¥–∞–ª–æ—Å—è –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ –∑ º—î–¥–Ω–∞–Ω–Ω—è –∑ AI —Å–µ—Ä–≤—ñ—Å–æ–º –ø—ñ—Å–ª—è –∫—ñ–ª—å–∫–æ—Ö —Å–ø—Ä–æ–±',
          code: 'AI_CONNECTION_FAILED',
          retries: maxRetries,
          timestamp: new Date().toISOString()
        });
      }
      // Stream processing with enhanced error handling
      try {
        // –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è —Ç–∞ –≤–∏–¥–æ–±—É–≤–∞–Ω–Ω—è JSON-–æ–±'—î–∫—Ç—ñ–≤
        const ALLOWED_TYPES = new Set(['highlight','summary','barometer']);

        // –î—ñ—Å—Ç–∞—î –∑ –±—É—Ñ–µ—Ä–∞ –≤—Å—ñ –ø–æ–≤–Ω—ñ JSON-–æ–±'—î–∫—Ç–∏ (brace-matching), –ø–æ–≤–µ—Ä—Ç–∞—î [objs, rest]
        function extractJsonObjects(buffer) {
          const out = [];
          let i = 0;
          const n = buffer.length;
          let depth = 0;
          let start = -1;
          let inStr = false;
          let esc = false;

          while (i < n) {
            const ch = buffer[i];

            if (inStr) {
              if (esc) { esc = false; }
              else if (ch === '\\') { esc = true; }
              else if (ch === '"') { inStr = false; }
              i++; continue;
            }

            if (ch === '"') { inStr = true; i++; continue; }

            if (ch === '{') {
              if (depth === 0) start = i;
              depth++;
            } else if (ch === '}') {
              depth--;
              if (depth === 0 && start >= 0) {
                const raw = buffer.slice(start, i + 1);
                out.push(raw);
                start = -1;
              }
            }

            i++;
          }

          const rest = depth === 0 ? '' : buffer.slice(start >= 0 ? start : n);
          return [out, rest];
        }

        // –°–∞–Ω—ñ—Ç–∏–∑–∞—Ü—ñ—è: –ø—Ä–∏–±—Ä–∞—Ç–∏ –±–µ–∫—Ç–∏–∫–∏, –º—ñ—Ç–∫–∏ ```json —Ç–∞ –∫–µ—Ä—ñ–≤–Ω—ñ —Å–∏–º–≤–æ–ª–∏ (–∫—Ä—ñ–º \n\t)
        const sanitizeChunk = (s) =>
          s
            .replace(/```(?:json)?/gi, '')
            .replace(/<\/?artifact[^>]*>/gi, '')
            .replace(/[\u0000-\u0008\u000B\u000C\u000E-\u001F]/g, '');

        let buffer = '';
        let chunkCount = 0;
        const maxChunks = 2000; // Prevent infinite processing
        
        for await (const part of stream) {
          if (++chunkCount > maxChunks) {
            logError(new Error('Too many chunks received from AI stream'), {
              chunkCount,
              bufferLength: buffer.length
            });
            break;
          }
          
          const delta = part.choices?.[0]?.delta?.content || '';
          if (!delta) continue;

          buffer += sanitizeChunk(delta);

          // –í–∏—Ç—è–≥—É—î–º–æ –≤—Å—ñ –∑–∞–≤–µ—Ä—à–µ–Ω—ñ JSON-–æ–±'—î–∫—Ç–∏ –∑ –±—É—Ñ–µ—Ä–∞
          const [rawObjs, rest] = extractJsonObjects(buffer);
          buffer = rest;

          for (const raw of rawObjs) {
            try {
              const obj = JSON.parse(raw);

              // –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –æ—á—ñ–∫—É–≤–∞–Ω—ñ —Ç–∏–ø–∏
              if (!obj || !ALLOWED_TYPES.has(obj.type)) continue;

              if (obj.type === 'highlight') {
                // Adjust highlight positions for chunk offset
                const adjustedHighlight = {
                  ...obj,
                  paragraph_index: obj.paragraph_index + (chunk.chunkIndex * 1000),
                  char_start: (obj.char_start || 0) + chunk.startChar,
                  char_end: (obj.char_end || 0) + chunk.startChar
                };
                rawHighlights.push(adjustedHighlight);
                sendLine(adjustedHighlight);
              } else if (obj.type === 'summary') {
                // Merge summaries from multiple chunks
                if (summaryObj) {
                  // Combine counts
                  Object.keys(obj.counts_by_category || {}).forEach(key => {
                    summaryObj.counts_by_category[key] = 
                      (summaryObj.counts_by_category[key] || 0) + (obj.counts_by_category[key] || 0);
                  });
                  // Combine patterns
                  summaryObj.top_patterns = [
                    ...(summaryObj.top_patterns || []),
                    ...(obj.top_patterns || [])
                  ];
                } else {
                  summaryObj = obj;
                }
              } else if (obj.type === 'barometer') {
                // Use the highest complexity score from all chunks
                if (!barometerObj || (obj.score || 0) > (barometerObj.score || 0)) {
                  barometerObj = obj;
                }
              }
            } catch (e) {
              // –¢–∏—Ö–æ —ñ–≥–Ω–æ—Ä—É—î–º–æ –±–∏—Ç—ñ –æ–±'—î–∫—Ç–∏
            }
          }
        }
      } catch (streamError) {
        clearInterval(heartbeat);
        
        logError(streamError, {
          context: 'Stream processing failed',
          clientId: finalClientId,
          textLength: text.length
        });
        
        if (!res.headersSent) {
          return res.status(503).json({
            error: '–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ AI —Å–µ—Ä–≤—ñ—Å—É',
            code: 'AI_STREAM_ERROR',
            timestamp: new Date().toISOString()
          });
        }
        return;
      }
      
      console.log(`üì¶ Completed chunk ${chunk.chunkIndex + 1}/${chunks.length}`);
    } // End of chunk processing loop

    // Remove artificial highlight limits to find all problems
    const merged = mergeOverlaps(rawHighlights, paragraphs);

    sendLine({ type: 'merged_highlights', items: merged });
    if (summaryObj) sendLine(summaryObj);
    if (barometerObj) sendLine(barometerObj);

    // Generate highlighted text for frontend display
    const highlightedText = generateHighlightedText(text, merged);

    // More accurate output token estimation based on highlights and summary
    let outputTokens = 500; // Base system response
    outputTokens += merged.length * 50; // ~50 tokens per highlight
    if (summaryObj) outputTokens += 300; // Summary tokens
    if (barometerObj) outputTokens += 100; // Barometer tokens
    
    totalTokensUsed += outputTokens;
    await addTokensAndCheck(outputTokens);
    
    // Log AI usage for monitoring
    logAIUsage(totalTokensUsed, MODEL, 'text_analysis');
    
    const analysisDuration = performance.now() - analysisStartTime;
    logPerformance('Complete Analysis', analysisDuration, {
      textLength: text.length,
      tokensUsed: totalTokensUsed,
      highlightsFound: merged.length,
      clientId: finalClientId
    });

    // Generate title for analysis
    const title = fileName
      ? `–ê–Ω–∞–ª—ñ–∑: ${fileName}`
      : `–ê–Ω–∞–ª—ñ–∑ –≤—ñ–¥ ${new Date().toLocaleDateString('uk-UA')}`;

    // Save to DB with client_id and highlighted text
    const result = await dbRun(
      `
      INSERT INTO analyses(
        client_id, title, source, original_filename, original_text, tokens_estimated,
        highlights_json, summary_json, barometer_json, highlighted_text, participants_filter
      ) VALUES ($1,$2,$3,$4,$5,$6,$7::jsonb,$8::jsonb,$9::jsonb,$10,$11::jsonb)
      RETURNING id
      `,
      [
        finalClientId,
        title,
        fileName ? 'file' : 'text',
        fileName || null,
        text.substring(0, 1000), // Save first 1000 chars for preview
        totalTokensUsed,
        JSON.stringify(merged),
        JSON.stringify(summaryObj || null),
        JSON.stringify(barometerObj || null),
        highlightedText,
        participantsFilter ? JSON.stringify(participantsFilter) : null,
      ]
    );

    const analysisId = result.rows[0].id;

    sendLine({
      type: 'analysis_saved',
      id: analysisId,
      client_id: finalClientId,
      original_text: text,
      participants_filter: participantsFilter
    });

    // Send complete signal to frontend
    sendLine({
      type: 'complete',
      analysis_id: analysisId
    });
    
    res.write('event: done\ndata: "ok"\n\n');
    res.end();
  } catch (err) {
    const analysisDuration = performance.now() - analysisStartTime;
    
    logError(err, {
      context: 'Analysis route error',
      duration: analysisDuration,
      tokensUsed: totalTokensUsed,
      ip: req.ip,
      userAgent: req.get('User-Agent')
    });
    
    try {
      if (!res.headersSent) {
        const isRateLimit = err.message.includes('–õ—ñ–º—ñ—Ç');
        const statusCode = isRateLimit ? 429 : 500;
        
        res.status(statusCode).json({ 
          error: err.message || '–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ –∞–Ω–∞–ª—ñ–∑—É',
          code: isRateLimit ? 'RATE_LIMIT_EXCEEDED' : 'ANALYSIS_ERROR',
          timestamp: new Date().toISOString()
        });
      } else {
        res.write(`event: error\ndata: ${JSON.stringify({
          error: err.message,
          timestamp: new Date().toISOString()
        })}\n\n`);
        res.end();
      }
    } catch (finalError) {
      logError(finalError, { context: 'Final error handler' });
    }
  }
});

export default r;